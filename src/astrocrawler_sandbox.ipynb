{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-10 21:46:55 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: scrapybot)\n",
      "2019-11-10 21:46:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.17763-SP0\n",
      "2019-11-10 21:46:55 [scrapy.crawler] INFO: Overridden settings: {'FEED_FORMAT': 'csv', 'FEED_URI': 'items.csv'}\n",
      "2019-11-10 21:46:55 [scrapy.extensions.telnet] INFO: Telnet Password: a37feb91bc543a5c\n",
      "2019-11-10 21:46:55 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-11-10 21:46:55 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-11-10 21:46:55 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-11-10 21:46:55 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-11-10 21:46:55 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-11-10 21:46:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-11-10 21:46:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-11-10 21:46:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://astrolibrary.org/interpretations/sun/> (referer: None)\n",
      "2019-11-10 21:46:56 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-11-10 21:46:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 235,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 25551,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 1.306852,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 11, 10, 13, 46, 56, 990702),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2019, 11, 10, 13, 46, 55, 683850)}\n",
      "2019-11-10 21:46:56 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Selector xpath='//div[@id=\"ris\"]' data='<div id=\"ris\">\\n<h2 id=\"aries\" class=\"...'>]\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import re\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "feature categories:\n",
    "1. PLANETS/POINTS in the SIGNS (e.g. Sun in Aries)\n",
    "    - there is usually 1 (but up to 2) feature/s per planet/point. e.g. sun may be close \n",
    "    to 2 different signs, so that the birth chart indicates the planet/point being in 2 \n",
    "    signs\n",
    "    \n",
    "2. PLANETS/POINTS in the HOUSES (e.g. Moon in 1st House)\n",
    "    - there are 1 or 2 features per planet/point.(Same explanation as above).\n",
    "\n",
    "3. SIGNS on the HOUSE CUSPS (e.g. Aries on 1st House Cusp)\n",
    "\n",
    "4. ASPECTS between two planets/points(for now the website seems to only have sun-related)\n",
    "    - e.g. Sun conjunct Mercury\n",
    "\n",
    "5. DOMINANCE/WEAKNESS OF ELEMENTS\n",
    "    - e.g. Dominance of FIRE, Weakness of Water\n",
    "\n",
    "6. DOMINANCE/WEAKNESS OF QUALITIES\n",
    "    - 3 QUALITIES: Mutable, Fixed, Cardinal\n",
    "    - same as for elements: the feature is based on a DOMINANCE or WEAKNESS of the QUALITY\n",
    "\n",
    "each feature is potentially associated with a description/interpretation of the person who\n",
    "bears this feature.\n",
    "\n",
    "BIG starting question: how do I structure the corpus?\n",
    "Maybe?:\n",
    "FEATURE_CAT | FEATURE | DESC\n",
    "===============================\n",
    "int         | Str     | Str\n",
    "\n",
    "So, we start w a given birth chart (generated by inputting birth info to the website),\n",
    "then from the birth chart we extract features. then we select rows based on the features.\n",
    "and compare the various descriptions for consistency/conflict.\n",
    "\n",
    "\"\"\"\n",
    "x = None\n",
    "\n",
    "class AstroSpider(scrapy.Spider):\n",
    "    name = 'astro_spider'\n",
    "    # start_urls = ['https://astrolibrary.org/interpretations/']\n",
    "    # start_urls = ['https://astrolibrary.org/interpretations/category/planets-in-signs/']\n",
    "    start_urls = ['https://astrolibrary.org/interpretations/sun/']\n",
    "    desc_links = []\n",
    "    data = []\n",
    "\n",
    "    # find category links from /interpretations\n",
    "    def parse2(self, response):\n",
    "        INTERPS_SELECTOR = '.interps-list-main'\n",
    "        LINK_SELECTOR = 'a ::attr(href)'\n",
    "        for interp in response.css(INTERPS_SELECTOR):\n",
    "            next_page = interp.css(LINK_SELECTOR).extract_first()\n",
    "            self.main_links.append(next_page)\n",
    "            if next_page:\n",
    "                yield scrapy.Request(\n",
    "                    response.urljoin(next_page),\n",
    "                    callback=self.page2\n",
    "                )\n",
    "                print(next_page)\n",
    "\n",
    "    # first find links to descriptions from /category/planets-in-signs\n",
    "    def parse1(self, response):\n",
    "        LIST_SELECTOR = '.listnice'\n",
    "        LI_SELECTOR = 'li'\n",
    "        LINK_SELECTOR = 'a ::attr(href)'\n",
    "\n",
    "        for nice in response.css(LIST_SELECTOR):\n",
    "            for li in nice.css(LI_SELECTOR):\n",
    "                next_page = li.css(LINK_SELECTOR).extract_first()\n",
    "\n",
    "                if next_page:\n",
    "                    # todo: exclude links that have the substring 'category'\n",
    "                    # we want links that bring us to the page of descriptions on a pair\n",
    "                    # of features.\n",
    "                    self.desc_links.append(next_page)\n",
    "                    yield scrapy.Request(\n",
    "                        response.urljoin(next_page),\n",
    "                        callback=self.nothing\n",
    "                    )\n",
    "                    print(next_page)\n",
    "\n",
    "    def parse(self, response):\n",
    "        # find <div id=\"ris\">\n",
    "        # split descriptions with <h2> tag\n",
    "        # for each <h2> tag, extract feature name.\n",
    "        # use each <h2> tag, extract descriptions from <p> tags, making sure to remove\n",
    "        # google ads.\n",
    "        global x\n",
    "        x = response.xpath('//div[@id=\"ris\"]')\n",
    "        print(x)\n",
    "        \n",
    "        # I've gotten the all the h2 and p tags in this long string. But i need to split\n",
    "        # it up.\n",
    "\n",
    "    def parse_pof(self, response):\n",
    "        # use this method if the preceding URL contained \"part-of-fortune\". This is due to\n",
    "        # the fact that the descriptions for part-of-fortune are organized and formatted\n",
    "        # differently.\n",
    "        pass\n",
    "\n",
    "    def nothing(self, response):\n",
    "        pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    process = CrawlerProcess(settings={\n",
    "        'FEED_FORMAT': 'csv',\n",
    "        'FEED_URI': 'items.csv'\n",
    "    })\n",
    "\n",
    "    process.crawl(AstroSpider)\n",
    "    process.start()  # the script will block here until the crawling is finished\n",
    "    # print(AstroSpider.data)\n",
    "    # todo: remove final links that have the substring 'category'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.xpath('//p | //h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Selector xpath='//p | //h2' data='<p>Aries sun natives can be inspirati...'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
